from fastapi import FastAPI
from pydantic import BaseModel
from nltk.corpus import wordnet as wn
import nltk
from nltk.stem import WordNetLemmatizer
import uvicorn
from transformers import pipeline
import spacy
import re
from sentence_transformers import SentenceTransformer, util


# Descargar datos de WordNet si no est√°n disponibles
nltk.download("wordnet")
nltk.download("omw-1.4")
embeddings_model = SentenceTransformer('all-mpnet-base-v2', device=0)
lemmatizer = WordNetLemmatizer()
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])  # Solo usamos POS tagging
ner_model = pipeline("ner", model="FacebookAI/xlm-roberta-large-finetuned-conll03-english", aggregation_strategy="simple", device=0)

app = FastAPI()

class QueryRequest(BaseModel):
    query: str


# Lista de conectores para dividir la query
CONNECTORS = {"in", "at", "with", "near", "on", "under", "over", "between", "and"}



def remove_photo_prefix(query: str):
    print(f"üîç Processing query: {query}")
    PREFIXES = [
        "photos ", "photos of", "images of", "pictures of", "I want to see images of", "show me pictures with", 
        "I'm looking for an image of", "I need a photo where", "an image with", "a photo that shows", 
        "I would like to explore pictures of", "photos resembling", "photos similar to", "photos inspired by", 
        "photos evoking", "photos reminiscent of", "photos capturing the essence of", "photos reflecting", 
        "photos resonating with", "images resembling", "images similar to", "images inspired by", 
        "images evoking", "images reminiscent of", "pictures resembling", "pictures similar to", "photos featuring", "images featuring",
        "pictures inspired by", "pictures reflecting", "pictures resonating with", "images for a project", "images for a series"
    ]
    PREFIX_EMBEDDINGS = embeddings_model.encode(PREFIXES, convert_to_tensor=True)
    words = query.lower().split()
    for n in range(2, 7):
        if len(words) >= n:
            segment = " ".join(words[:n])
            segment_embedding = embeddings_model.encode(segment, convert_to_tensor=True)
            similarities = util.pytorch_cos_sim(segment_embedding, PREFIX_EMBEDDINGS)[0]
            if any(similarity.item() > 0.8 for similarity in similarities):
                print(f"‚úÖ Prefix detected and removed: {segment}")
                return " ".join(query.split()[n:]).strip()
    print("‚ùå No irrelevant prefix detected.")
    return query

def remove_dumb_connectors(query: str):
    DUMB_CONNECTORS = {"a", "an", "the", "some", "any", "this", "that", "these", "those", "my", "your", "his", "her", "its", "our", "their", "one", "two", "few", "several", "many", "much", "each", "every", "either", "neither", "another", "such"}
    
    words = query.split()
    filtered_words = [word for word in words if word.lower() not in DUMB_CONNECTORS]
    
    return " ".join(filtered_words)


# Funci√≥n de respaldo con Spacy para etiquetar palabras desconocidas
def get_pos_spacy_no_context(word):
    doc = nlp(word)
    pos = doc[0].pos_
    return pos if pos in ["ADJ", "NOUN", "VERB", "ADV", "ADP"] else "UNKNOWN"

def get_pos_spacy(word: str, sentence: str = None) -> str:
    if sentence:
        doc = nlp(sentence)
        # Buscar el token que coincida con la palabra
        for token in doc:
            if token.text.lower() == word.lower():
                pos = token.pos_
                final_pos = pos if pos in ["ADJ", "NOUN", "VERB", "ADV", "ADP"] else "UNKNOWN"
                return final_pos
    else:
        print("DEBUG: No se proporcion√≥ oraci√≥n; analizando la palabra de forma aislada.")
    
    # An√°lisis aislado
    doc = nlp(word)
    pos = doc[0].pos_
    final_pos = pos if pos in ["ADJ", "NOUN", "VERB", "ADV", "ADP"] else "UNKNOWN"
    return final_pos

# Funciones para determinar la categor√≠a gramatical de una palabra con WordNet y Spacy como respaldo
def is_adjective(word, query = None):
    lemma = lemmatizer.lemmatize(word, pos='a')
    return any(ss.pos() == 'a' for ss in wn.synsets(lemma)) or get_pos_spacy(word, query) == "ADJ"

def is_noun(word, query):
    lemma = lemmatizer.lemmatize(word, pos='n')
    return any(ss.pos() == 'n' for ss in wn.synsets(lemma)) or get_pos_spacy(word, query) == "NOUN"

def is_verb(word, query):
    lemma = lemmatizer.lemmatize(word, pos='v')
    return any(ss.pos() == 'v' for ss in wn.synsets(lemma)) or get_pos_spacy(word, query) == "VERB"

def is_adverb(word, query):
    lemma = lemmatizer.lemmatize(word, pos='r')
    return any(ss.pos() == 'r' for ss in wn.synsets(lemma)) or get_pos_spacy(word, query) == "ADV"

def is_preposition(word, query):
    return word in {"of"}


# Lista de estructuras de segmentos a detectar
patterns = [
    ("ADJ_NOUN", [is_adjective, is_noun]),  # lazy girl
    ("NOUN_ADJ", [is_noun, is_adjective]),  # market sunny
    ("ADJ_NOUN_VERB", [is_adjective, is_noun, is_verb]),  # lazy girl sleeping
    ("NOUN_PREP_NOUN", [is_noun, is_preposition, is_noun]),  # concept of chaos
    ("NOUN_VERB", [is_noun, is_verb]),  # girl sleeping
    ("NOUN_NOUN", [is_noun, is_noun]),  # farm animals
    ("NOUN_VERB_CD", [is_noun, is_verb, is_noun]),  # girl eating icecream
    ("NOUN_VERB_ADJ_NOUN", [is_noun, is_verb, is_adjective, is_noun]),  # girl eating nice icecream
    ("NOUN_VERB_ADJ", [is_noun, is_verb, is_adjective]),  # girl working hard
    ("VERB_ALONE", [is_verb]),  # sleeping
    ("NOUN_ALONE", [is_noun]),  # girl
]

def split_query_with_connectors(query: str):
    words = query.lower().replace(",", "").split()
    segments = []
    current_segment = []
    
    for word in words:
        if word in CONNECTORS:
            if current_segment:
                segments.append(" ".join(current_segment))
                current_segment = []
        else:
            current_segment.append(word)
    
    if current_segment:
        segments.append(" ".join(current_segment))

    print(f"\nSegmentos por conectores: {segments}")

    
    return segments

def block_er_entities(query: str, use_model: bool = False) -> str:
    """
    Procesa la query para identificar entidades nombradas y las bloquea envolvi√©ndolas en corchetes,
    de modo que no se procesen en etapas posteriores.
    
    Par√°metros:
      - query: cadena de texto original.
      - use_model: si es True se usar√° el modelo de HuggingFace (ner_model, previamente instanciado) para detectar entidades;
                   si es False se aplicar√° una heur√≠stica que detecta secuencias de dos o m√°s palabras que inician en may√∫scula.
    
    Retorna:
      La query con las entidades detectadas bloqueadas entre corchetes.
    """
    if use_model:
        # Se asume que ner_model est√° definido globalmente
        entities = ner_model(query)
        # Ordenar las entidades de forma descendente seg√∫n su posici√≥n para evitar problemas al reemplazar
        for ent in sorted(entities, key=lambda x: x['start'], reverse=True):
            start, end = ent['start'], ent['end']
            query = query[:start] + f"[{query[start:end]}]" + query[end:]
        return query
    else:
        # Heur√≠stica: buscamos secuencias de dos o m√°s palabras que comienzan con may√∫scula.
        pattern = r'\b(?:[A-Z][a-z]*(?:\s+[A-Z][a-z]*)+)\b'
        return re.sub(pattern, lambda m: f"[{m.group(0)}]", query)

def preprocess_query(query: str):
    no_prefix_query = remove_photo_prefix(query)
    print(f" No-prefix query: {no_prefix_query}")

    blocked_er_query = block_er_entities(no_prefix_query)

    clean_query = remove_dumb_connectors(blocked_er_query)
    print(f" Clean query: {clean_query}")

    splitted_query = split_query_with_connectors(clean_query)
    return splitted_query, no_prefix_query

def segment_query_v2(query: str) -> str:
    preprocessed_segments, no_prefix_query = preprocess_query(query)
    all_segments = set()
    
    for segment in preprocessed_segments:
        words = segment.split()
        
        print(f"\nProcesando segmento: {segment}")
        
        i = 0
        while i < len(words):
            for name, pattern in patterns:
                if i + len(pattern) <= len(words):
                    word_segment = words[i:i+len(pattern)]
                    if all(pattern[j](word_segment[j], 'photos of ' + no_prefix_query) for j in range(len(pattern))):
                        segment_text = " ".join(word_segment)
                        print(f"    ‚úÖ Match: {segment_text} ({name})")
                        all_segments.add(segment_text)
            i += 1
    
    return " | ".join(all_segments)

def remove_contained_segments_simple(segments_str: str) -> list[str]:
    # Separamos la cadena por "|" y limpiamos espacios
    segments = [s.strip() for s in segments_str.split("|") if s.strip()]
    # Eliminamos duplicados manteniendo el orden
    segments = list(dict.fromkeys(segments))
    
    final_segments = []
    for seg in segments:
        if not any(seg != other and seg in other for other in segments):
            final_segments.append(seg)
    return final_segments

def minimal_segment_cover(segments_str: str) -> list[str]:
    # Separamos la cadena y eliminamos duplicados
    segments = [s.strip() for s in segments_str.split("|") if s.strip()]
    segments = list(dict.fromkeys(segments))
    
    # Asociamos cada segmento a su conjunto de palabras (en min√∫sculas)
    seg_to_words = {seg: set(seg.split()) for seg in segments}
    # Universo: todas las palabras presentes en alg√∫n segmento
    universe = set().union(*seg_to_words.values())
    
    best_cover = None
    best_cover_size = float('inf')
    best_cover_total_words = float('inf')
    n = len(segments)
    
    # Recorremos todos los subconjuntos de segmentos (brute force, n peque√±o)
    for i in range(1, 1 << n):
        subset = [segments[j] for j in range(n) if (i >> j) & 1]
        covered = set()
        for seg in subset:
            covered |= seg_to_words[seg]
        if covered == universe:
            subset_size = len(subset)
            total_words = sum(len(seg.split()) for seg in subset)
            # Prioriza el menor n√∫mero de segmentos, y en empate, el de menos palabras
            if subset_size < best_cover_size or (subset_size == best_cover_size and total_words < best_cover_total_words):
                best_cover = subset
                best_cover_size = subset_size
                best_cover_total_words = total_words

    if best_cover is None:
        return segments
    # Ordenamos seg√∫n el orden de aparici√≥n original
    best_cover.sort(key=lambda s: segments.index(s))
    return best_cover


@app.post("/segment-query")
def segment_query(request: QueryRequest):
    result = segment_query_v2(request.query)
    filtered_result = minimal_segment_cover(result)
    return {"segments": result, "filtered_segments": filtered_result}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5000, reload=True)